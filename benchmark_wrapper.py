"""
AIGEN BioAgent - í†µí•© ë²¤ì¹˜ë§ˆí¬ ë˜í¼
=====================================
Phase 1: Biomni-Eval1, LAB-Bench, BioML-Bench í†µí•© ì¸í„°í˜ì´ìŠ¤

ì‘ì„±ì¼: 2026-02-05
ë‹´ë‹¹ì: Hoony (Phase 1 ë¦¬ë“œ)
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from enum import Enum
from datasets import load_dataset
import time


# ============================================
# 1. ê³µí†µ ë°ì´í„° êµ¬ì¡°
# ============================================

class BenchmarkType(Enum):
    """ë²¤ì¹˜ë§ˆí¬ ì¢…ë¥˜"""
    BIOMNI_EVAL1 = "biomni_eval1"
    LAB_BENCH = "lab_bench"
    BIOML_BENCH = "bioml_bench"


@dataclass
class BenchmarkTask:
    """ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬ ê³µí†µ êµ¬ì¡°"""
    task_id: str                    # ê³ ìœ  ID
    benchmark: BenchmarkType        # ë²¤ì¹˜ë§ˆí¬ ì¢…ë¥˜
    question: str                   # ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸
    ground_truth: Any               # ì •ë‹µ
    task_type: str                  # íƒœìŠ¤í¬ ìœ í˜• (ì˜ˆ: LitQA, ProtocolQA ë“±)
    metadata: Dict[str, Any] = field(default_factory=dict)  # ì¶”ê°€ ì •ë³´


@dataclass
class AgentResponse:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ êµ¬ì¡°"""
    task_id: str                    # íƒœìŠ¤í¬ ID
    answer: Any                     # ì—ì´ì „íŠ¸ ë‹µë³€
    trajectory: List[Dict] = field(default_factory=list)  # ì‹¤í–‰ ê³¼ì • ë¡œê·¸
    latency_ms: float = 0.0         # ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
    token_usage: Dict[str, int] = field(default_factory=dict)  # í† í° ì‚¬ìš©ëŸ‰
    errors: List[str] = field(default_factory=list)  # ë°œìƒí•œ ì—ëŸ¬ë“¤


@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ êµ¬ì¡°"""
    task_id: str                    # íƒœìŠ¤í¬ ID
    score: float                    # ì ìˆ˜ (0.0 ~ 1.0)
    is_correct: bool                # ì •ë‹µ ì—¬ë¶€
    ground_truth: Any               # ì •ë‹µ
    predicted: Any                  # ì˜ˆì¸¡ê°’
    details: Dict[str, Any] = field(default_factory=dict)  # ìƒì„¸ ì •ë³´


# ============================================
# 2. ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
# ============================================

class BaseBenchmarkWrapper(ABC):
    """ë²¤ì¹˜ë§ˆí¬ ë˜í¼ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.tasks: List[BenchmarkTask] = []
        self.loaded = False
    
    @property
    @abstractmethod
    def benchmark_type(self) -> BenchmarkType:
        """ë²¤ì¹˜ë§ˆí¬ ì¢…ë¥˜ ë°˜í™˜"""
        pass
    
    @property
    @abstractmethod
    def benchmark_name(self) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ ë°˜í™˜"""
        pass
    
    @abstractmethod
    def load_tasks(self, **kwargs) -> List[BenchmarkTask]:
        """íƒœìŠ¤í¬ ë¡œë”©"""
        pass
    
    @abstractmethod
    def evaluate(self, task: BenchmarkTask, response: AgentResponse) -> EvaluationResult:
        """ë‹¨ì¼ íƒœìŠ¤í¬ í‰ê°€"""
        pass
    
    def evaluate_batch(self, responses: List[AgentResponse]) -> List[EvaluationResult]:
        """ë°°ì¹˜ í‰ê°€"""
        task_map = {t.task_id: t for t in self.tasks}
        results = []
        for resp in responses:
            if resp.task_id in task_map:
                result = self.evaluate(task_map[resp.task_id], resp)
                results.append(result)
        return results
    
    def get_summary(self, results: List[EvaluationResult]) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½"""
        if not results:
            return {"total": 0, "accuracy": 0.0, "average_score": 0.0}
        
        total = len(results)
        correct = sum(1 for r in results if r.is_correct)
        avg_score = sum(r.score for r in results) / total
        
        return {
            "benchmark": self.benchmark_name,
            "total": total,
            "correct": correct,
            "accuracy": correct / total,
            "average_score": avg_score
        }


# ============================================
# 3. Biomni-Eval1 ë˜í¼
# ============================================

class BiomniEval1Wrapper(BaseBenchmarkWrapper):
    """Biomni-Eval1 ë²¤ì¹˜ë§ˆí¬ ë˜í¼"""
    
    DATASET_PATH = "biomni/Eval1"
    SPLIT = "test"  # ë¬¸ì„œì—ëŠ” trainì´ì§€ë§Œ ì‹¤ì œëŠ” test
    
    @property
    def benchmark_type(self) -> BenchmarkType:
        return BenchmarkType.BIOMNI_EVAL1
    
    @property
    def benchmark_name(self) -> str:
        return "Biomni-Eval1"
    
    def load_tasks(self, limit: Optional[int] = None) -> List[BenchmarkTask]:
        """
        Biomni-Eval1 íƒœìŠ¤í¬ ë¡œë”©
        
        Args:
            limit: ë¡œë”©í•  ìµœëŒ€ íƒœìŠ¤í¬ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            BenchmarkTask ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¥ {self.benchmark_name} ë¡œë”© ì¤‘...")
        dataset = load_dataset(self.DATASET_PATH, split=self.SPLIT)
        
        self.tasks = []
        for i, item in enumerate(dataset):
            if limit and i >= limit:
                break
            
            task = BenchmarkTask(
                task_id=item["instance_id"],
                benchmark=self.benchmark_type,
                question=item["prompt"],
                ground_truth=item["answer"],
                task_type=item["task_name"],
                metadata={
                    "task_instance_id": item["task_instance_id"],
                    "split": item["split"]
                }
            )
            self.tasks.append(task)
        
        self.loaded = True
        print(f"âœ… {len(self.tasks)}ê°œ íƒœìŠ¤í¬ ë¡œë”© ì™„ë£Œ")
        return self.tasks
    
    def evaluate(self, task: BenchmarkTask, response: AgentResponse) -> EvaluationResult:
        """
        Biomni-Eval1 í‰ê°€: Exact Match
        """
        predicted = str(response.answer).strip().lower()
        ground_truth = str(task.ground_truth).strip().lower()
        
        is_correct = predicted == ground_truth
        score = 1.0 if is_correct else 0.0
        
        return EvaluationResult(
            task_id=task.task_id,
            score=score,
            is_correct=is_correct,
            ground_truth=task.ground_truth,
            predicted=response.answer,
            details={
                "evaluation_method": "exact_match",
                "task_type": task.task_type
            }
        )


# ============================================
# 4. LAB-Bench ë˜í¼
# ============================================

class LabBenchWrapper(BaseBenchmarkWrapper):
    """LAB-Bench ë²¤ì¹˜ë§ˆí¬ ë˜í¼"""
    
    DATASET_PATH = "futurehouse/lab-bench"
    SPLIT = "train"  # ë¬¸ì„œì—ëŠ” testì´ì§€ë§Œ ì‹¤ì œëŠ” train
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬
    CATEGORIES = [
        "CloningScenarios", "DbQA", "FigQA", "LitQA2",
        "ProtocolQA", "SeqQA", "SuppQA", "TableQA"
    ]
    
    def __init__(self, categories: Optional[List[str]] = None):
        """
        Args:
            categories: ë¡œë”©í•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        """
        super().__init__()
        self.selected_categories = categories or self.CATEGORIES
    
    @property
    def benchmark_type(self) -> BenchmarkType:
        return BenchmarkType.LAB_BENCH
    
    @property
    def benchmark_name(self) -> str:
        return "LAB-Bench"
    
    def load_tasks(self, limit_per_category: Optional[int] = None) -> List[BenchmarkTask]:
        """
        LAB-Bench íƒœìŠ¤í¬ ë¡œë”©
        
        Args:
            limit_per_category: ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ íƒœìŠ¤í¬ ìˆ˜
        
        Returns:
            BenchmarkTask ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¥ {self.benchmark_name} ë¡œë”© ì¤‘...")
        self.tasks = []
        
        for category in self.selected_categories:
            try:
                print(f"   - {category} ë¡œë”© ì¤‘...")
                dataset = load_dataset(self.DATASET_PATH, category, split=self.SPLIT)
                
                for i, item in enumerate(dataset):
                    if limit_per_category and i >= limit_per_category:
                        break
                    
                    # ì„ íƒì§€ êµ¬ì„±: ideal + distractors
                    options = [item["ideal"]] + item["distractors"]
                    
                    task = BenchmarkTask(
                        task_id=item["id"],
                        benchmark=self.benchmark_type,
                        question=item["question"],
                        ground_truth=item["ideal"],  # ì •ë‹µì€ ideal
                        task_type=category,
                        metadata={
                            "options": options,
                            "distractors": item["distractors"],
                            "source": item.get("source", ""),
                            "subtask": item.get("subtask", ""),
                            "is_opensource": item.get("is_opensource", False)
                        }
                    )
                    self.tasks.append(task)
                    
            except Exception as e:
                print(f"   âš ï¸ {category} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        self.loaded = True
        print(f"âœ… {len(self.tasks)}ê°œ íƒœìŠ¤í¬ ë¡œë”© ì™„ë£Œ")
        return self.tasks
    
    def evaluate(self, task: BenchmarkTask, response: AgentResponse) -> EvaluationResult:
        """
        LAB-Bench í‰ê°€: ì •ë‹µ(ideal)ê³¼ ë§¤ì¹­
        """
        predicted = str(response.answer).strip()
        ground_truth = str(task.ground_truth).strip()
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜, ì •ë‹µì´ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì •ë‹µ
        is_correct = (predicted.lower() == ground_truth.lower() or 
                      ground_truth.lower() in predicted.lower())
        score = 1.0 if is_correct else 0.0
        
        return EvaluationResult(
            task_id=task.task_id,
            score=score,
            is_correct=is_correct,
            ground_truth=task.ground_truth,
            predicted=response.answer,
            details={
                "evaluation_method": "ideal_match",
                "task_type": task.task_type,
                "options": task.metadata.get("options", [])
            }
        )


# ============================================
# 5. BioML-Bench ë˜í¼ (ê¸°ë³¸ êµ¬ì¡°)
# ============================================

class BioMLBenchWrapper(BaseBenchmarkWrapper):
    """BioML-Bench ë²¤ì¹˜ë§ˆí¬ ë˜í¼ (GitHub ê¸°ë°˜)"""
    
    GITHUB_REPO = "https://github.com/BioML-bench/bioml-bench"
    
    # 24ê°œ íƒœìŠ¤í¬ ë„ë©”ì¸
    DOMAINS = [
        "protein_engineering",
        "drug_discovery", 
        "genomics",
        "clinical_prediction"
    ]
    
    @property
    def benchmark_type(self) -> BenchmarkType:
        return BenchmarkType.BIOML_BENCH
    
    @property
    def benchmark_name(self) -> str:
        return "BioML-Bench"
    
    def load_tasks(self, **kwargs) -> List[BenchmarkTask]:
        """
        BioML-Bench íƒœìŠ¤í¬ ë¡œë”©
        
        Note: GitHub ê¸°ë°˜ì´ë¼ ë³„ë„ ë‹¤ìš´ë¡œë“œ í•„ìš”
        í˜„ì¬ëŠ” í”Œë ˆì´ìŠ¤í™€ë” êµ¬í˜„
        """
        print(f"ğŸ“¥ {self.benchmark_name} ë¡œë”© ì¤‘...")
        print(f"   âš ï¸ GitHub ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ - ë³„ë„ ì„¤ì • í•„ìš”")
        print(f"   ğŸ“ ì €ì¥ì†Œ: {self.GITHUB_REPO}")
        
        # í”Œë ˆì´ìŠ¤í™€ë”: ì‹¤ì œ êµ¬í˜„ ì‹œ GitHubì—ì„œ ë°ì´í„° ë¡œë”©
        self.tasks = []
        self.loaded = True
        
        return self.tasks
    
    def evaluate(self, task: BenchmarkTask, response: AgentResponse) -> EvaluationResult:
        """
        BioML-Bench í‰ê°€: ML ë©”íŠ¸ë¦­ ê¸°ë°˜ (AUROC, Spearman ë“±)
        """
        # í”Œë ˆì´ìŠ¤í™€ë”: ì‹¤ì œ êµ¬í˜„ ì‹œ íƒœìŠ¤í¬ë³„ ë©”íŠ¸ë¦­ ì ìš©
        return EvaluationResult(
            task_id=task.task_id,
            score=0.0,
            is_correct=False,
            ground_truth=task.ground_truth,
            predicted=response.answer,
            details={
                "evaluation_method": "ml_metrics",
                "note": "BioML-Bench í‰ê°€ëŠ” íƒœìŠ¤í¬ë³„ ML ë©”íŠ¸ë¦­ í•„ìš”"
            }
        )


# ============================================
# 6. í†µí•© ë²¤ì¹˜ë§ˆí¬ ë§¤ë‹ˆì €
# ============================================

class BenchmarkManager:
    """ë²¤ì¹˜ë§ˆí¬ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.wrappers: Dict[BenchmarkType, BaseBenchmarkWrapper] = {}
    
    def register(self, wrapper: BaseBenchmarkWrapper):
        """ë˜í¼ ë“±ë¡"""
        self.wrappers[wrapper.benchmark_type] = wrapper
        print(f"ğŸ“Œ {wrapper.benchmark_name} ë“±ë¡ë¨")
    
    def load_all(self, **kwargs):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ë¡œë”©"""
        for wrapper in self.wrappers.values():
            wrapper.load_tasks(**kwargs)
    
    def get_all_tasks(self) -> List[BenchmarkTask]:
        """ëª¨ë“  íƒœìŠ¤í¬ ë°˜í™˜"""
        all_tasks = []
        for wrapper in self.wrappers.values():
            all_tasks.extend(wrapper.tasks)
        return all_tasks
    
    def evaluate_all(self, responses: List[AgentResponse]) -> Dict[str, Any]:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
        results = {}
        
        # ì‘ë‹µì„ ë²¤ì¹˜ë§ˆí¬ë³„ë¡œ ë¶„ë¥˜
        for wrapper in self.wrappers.values():
            task_ids = {t.task_id for t in wrapper.tasks}
            relevant_responses = [r for r in responses if r.task_id in task_ids]
            
            if relevant_responses:
                eval_results = wrapper.evaluate_batch(relevant_responses)
                results[wrapper.benchmark_name] = wrapper.get_summary(eval_results)
        
        return results


# ============================================
# 7. í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª ë²¤ì¹˜ë§ˆí¬ ë˜í¼ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. Biomni-Eval1 í…ŒìŠ¤íŠ¸
    print("\n[1] Biomni-Eval1 ë˜í¼ í…ŒìŠ¤íŠ¸")
    biomni = BiomniEval1Wrapper()
    tasks = biomni.load_tasks(limit=5)
    
    print(f"\n   ìƒ˜í”Œ íƒœìŠ¤í¬:")
    print(f"   - ID: {tasks[0].task_id}")
    print(f"   - íƒ€ì…: {tasks[0].task_type}")
    print(f"   - ì§ˆë¬¸: {tasks[0].question[:100]}...")
    print(f"   - ì •ë‹µ: {tasks[0].ground_truth}")
    
    # í‰ê°€ í…ŒìŠ¤íŠ¸ (ê°€ìƒ ì‘ë‹µ)
    fake_response = AgentResponse(
        task_id=tasks[0].task_id,
        answer=tasks[0].ground_truth  # ì •ë‹µìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    )
    result = biomni.evaluate(tasks[0], fake_response)
    print(f"\n   í‰ê°€ ê²°ê³¼: score={result.score}, correct={result.is_correct}")
    
    # 2. LAB-Bench í…ŒìŠ¤íŠ¸
    print("\n" + "-" * 60)
    print("[2] LAB-Bench ë˜í¼ í…ŒìŠ¤íŠ¸")
    labbench = LabBenchWrapper(categories=["LitQA2"])
    tasks = labbench.load_tasks(limit_per_category=5)
    
    print(f"\n   ìƒ˜í”Œ íƒœìŠ¤í¬:")
    print(f"   - ID: {tasks[0].task_id}")
    print(f"   - íƒ€ì…: {tasks[0].task_type}")
    print(f"   - ì§ˆë¬¸: {tasks[0].question[:100]}...")
    print(f"   - ì •ë‹µ: {tasks[0].ground_truth[:50]}...")
    
    # 3. í†µí•© ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
    print("\n" + "-" * 60)
    print("[3] í†µí•© ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    manager = BenchmarkManager()
    manager.register(BiomniEval1Wrapper())
    manager.register(LabBenchWrapper(categories=["LitQA2"]))
    
    print("\n" + "=" * 60)
    print("âœ… ë˜í¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
